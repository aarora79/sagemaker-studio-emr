{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Python Packages to Existing EMR Clusters\n",
    "\n",
    "## Notebook Scope Libraries\n",
    "Notebook-scoped libraries provide you the following benefits:\n",
    "\n",
    "* Runtime installation – You can import your favorite Python libraries from PyPI repositories and install them on your remote cluster (driver and executors) on the fly when you need them. These libraries are instantly available to your Spark runtime environment. There is no need to restart the notebook session or recreate your cluster.\n",
    "* Dependency isolation – The libraries you install using EMR Notebooks are isolated to your notebook session and don’t interfere with bootstrapped cluster libraries or libraries installed from other notebook sessions. These notebook-scoped libraries take precedence over bootstrapped libraries. Multiple notebook users can import their preferred version of the library and use it without dependency clashes on the same cluster.\n",
    "* Portable library environment – The library package installation happens from your notebook file. This allows you to recreate the library environment when you switch the notebook to a different cluster by re-executing the notebook code. At the end of the notebook session, the libraries you install through EMR Notebooks are automatically removed from the hosting EMR cluster.\n",
    "\n",
    "#### This functionality is available for clusters running EMR release >= 5.26.0. More info can be found in the [docs here](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks-installing-libraries-and-kernels.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To enable notebook scoped libraries, we must set the configuration to use a virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python3\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-cfn-bootstrap (2.0)\n",
      "beautifulsoup4 (4.9.3)\n",
      "boto (2.49.0)\n",
      "boto3 (1.18.63)\n",
      "botocore (1.21.65)\n",
      "certifi (2021.10.8)\n",
      "charset-normalizer (2.0.9)\n",
      "click (8.0.1)\n",
      "cycler (0.11.0)\n",
      "Cython (0.29.24)\n",
      "docutils (0.14)\n",
      "idna (3.3)\n",
      "jmespath (0.10.0)\n",
      "joblib (1.0.1)\n",
      "kiwisolver (1.3.2)\n",
      "lockfile (0.11.0)\n",
      "lxml (4.6.3)\n",
      "matplotlib (3.4.3)\n",
      "mysqlclient (1.4.2)\n",
      "nltk (3.6.2)\n",
      "nose (1.3.4)\n",
      "numpy (1.21.2)\n",
      "pandas (1.2.5)\n",
      "Pillow (8.4.0)\n",
      "pip (9.0.1)\n",
      "py-dateutil (2.2)\n",
      "pyparsing (3.0.6)\n",
      "pystache (0.5.4)\n",
      "python-daemon (2.2.3)\n",
      "python-dateutil (2.8.2)\n",
      "python37-sagemaker-pyspark (1.4.1)\n",
      "pytz (2021.1)\n",
      "PyYAML (5.4.1)\n",
      "regex (2021.8.3)\n",
      "requests (2.26.0)\n",
      "s3transfer (0.5.0)\n",
      "setuptools (28.8.0)\n",
      "simplejson (3.2.0)\n",
      "six (1.13.0)\n",
      "tqdm (4.62.1)\n",
      "urllib3 (1.26.7)\n",
      "wheel (0.29.0)\n",
      "windmill (1.6)\n",
      "\n",
      "DEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\n",
      "You are using pip version 9.0.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command."
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice that there is no \"translate\" package available on our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install translate from given PyPI repository\n",
    "sc.install_pypi_package(\"translate\", \"https://pypi.org/simple\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from translate import Translator\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "@udf\n",
    "def translate_to_german(sentence):\n",
    "    translator= Translator(to_lang=\"de\")\n",
    "    return translator.translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|TaskID|Sentence       |\n",
      "+------+---------------+\n",
      "|1     |This is a pen  |\n",
      "|2     |This is a chair|\n",
      "+------+---------------+"
     ]
    }
   ],
   "source": [
    "columns = [\"TaskID\",\"Sentence\"]\n",
    "data = [(\"1\", \"This is a pen\"),\n",
    "    (\"2\", \"This is a chair\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|TaskID|Sentence           |\n",
      "+------+-------------------+\n",
      "|1     |Das ist ein Gehege.|\n",
      "|2     |Das ist ein Stuhl  |\n",
      "+------+-------------------+"
     ]
    }
   ],
   "source": [
    "df.select(col(\"TaskID\"), \\\n",
    "    translate_to_german(col(\"Sentence\")).alias(\"Sentence\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "PySpark (SparkMagic)",
   "language": "python",
   "name": "pysparkkernel__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/sagemaker-sparkmagic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}