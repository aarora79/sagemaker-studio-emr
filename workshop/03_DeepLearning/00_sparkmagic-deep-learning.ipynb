{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMR Data Prep + SageMaker Deep Learning\n",
    "\n",
    "This notebook is tested using `Studio SparkMagic - PySpark Kernel` running on a `ml.t3.medium` instance and connected to an EMR clsuter with an `m5.xlarge` Master node and 2 `m5.xlarge` Core nodes. Please ensure that you see `PySpark (SparkMagic)` in the top right on your notebook.\n",
    "\n",
    "In this 3 part notebook lesson, we'll see how to use EMR for data prep and serialization to S3. Next we'll prototype a deep learning architecture using SageMaker Studio notebooks, and lastly we'll scale the training using SageMaker ephemeral training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext sagemaker_studio_analytics_extension.magics\n",
    "# %sm_analytics emr connect --cluster_id j-xxxxxxxxxxxx --auth-type None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the public NYC Taxi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "!aws s3 ls \"s3://nyc-tlc/trip data/green\" --human-readable | grep green_tripdata_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3://nyc-tlc/trip data/green_tripdata_2016*.csv\", header=True, inferSchema=True, timestampFormat='yyyy-MM-dd HH:mm:ss').cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pretty\n",
    "from pyspark.sql.functions import col, dayofweek, month, hour\n",
    "df_dt = df.select(dayofweek(col('lpep_pickup_datetime')).alias('day_of_week'),\n",
    "                   month(col('lpep_pickup_datetime')).alias('month'),\n",
    "                   hour(col('lpep_pickup_datetime')).alias('hour'),\n",
    "                   col(\"Pickup_latitude\").alias(\"pickup_latitude\"),\n",
    "                   col(\"Pickup_longitude\").alias(\"pickup_longitude\"),\n",
    "                   col(\"Dropoff_latitude\").alias(\"dropoff_latitude\"),\n",
    "                   col(\"Dropoff_latitude\").alias(\"dropoff_longitude\"),\n",
    "                   col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                   col(\"Fare_amount\").alias(\"fare_amount\")\n",
    "                  )\n",
    "df_dt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Data Clean Up at Scale on the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt = df_dt[\n",
    "    (df_dt.fare_amount > 0)\n",
    "    & (df_dt.fare_amount < 200)    \n",
    "]\n",
    "df_dt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pretty\n",
    "df_dt = df_dt[\n",
    "    (df_dt.pickup_latitude != 0)    \n",
    "]\n",
    "df_dt.show()\n",
    "df_dt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = df_dt.randomSplit([0.8, 0.2], seed=42)\n",
    "val_df, test_df = val_df.randomSplit([0.05, 0.95], seed=42)\n",
    "\n",
    "print(\"Train Count:\", train_df.count())\n",
    "print(\"Validation Count:\", val_df.count())\n",
    "print(\"Test Count:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "data_bucket = f\"{bucket}/nyc-taxi/data/processed\"\n",
    "print(data_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i data_bucket -t str -n data_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.write.csv(f\"s3://{data_bucket}/train\", mode='overwrite')\n",
    "test_df.write.csv(f\"s3://{data_bucket}/test\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data location for next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store data_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "PySpark (SparkMagic)",
   "language": "python",
   "name": "pysparkkernel__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/sagemaker-sparkmagic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
