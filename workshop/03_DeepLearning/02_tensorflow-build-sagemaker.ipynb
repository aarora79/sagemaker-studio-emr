{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Training at Scale\n",
    "\n",
    "This notebook is tested using `Data Science - Python 3 Kernel` running on a `ml.t3.medium` instance. Please ensure that you use `Python 3 (Data Science)` in the top right on your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don't see anything printed then it's probably the first time you are running the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure updated SageMaker SDK version\n",
    "%pip install -U -q sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SageMaker Experiments Library\n",
    "# https://github.com/aws/sagemaker-experiments\n",
    "%pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tf_train.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.experimental import LinearModel, WideDeepModel\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "\n",
    "    # data directories\n",
    "    parser.add_argument(\"--training\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--testing\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n",
    "\n",
    "    # model directory: we will use the default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_train_data(train_dir, batch_size):\n",
    "\n",
    "    def pack(features, label):\n",
    "        linear_features = [tf.cast(features['day_of_week'], tf.float32), tf.cast(features['month'], tf.float32),\n",
    "                           tf.cast(features['hour'], tf.float32), features[\"trip_distance\"]]\n",
    "        \n",
    "        dnn_features = [features['pickup_latitude'], features['pickup_longitude'], features['dropoff_latitude'], features['dropoff_longitude'], features[\"trip_distance\"]]\n",
    "        return (tf.stack(linear_features, axis=-1), tf.stack(dnn_features, axis=-1)), label\n",
    "\n",
    "    \n",
    "    column_headers = [\"day_of_week\",\"month\",\"hour\",\"pickup_latitude\",\"pickup_longitude\",\n",
    "                      \"dropoff_latitude\",\"dropoff_longitude\",\"trip_distance\",\"fare_amount\"]\n",
    "\n",
    "    ds = tf.data.experimental.make_csv_dataset(tf.io.gfile.glob(train_dir + '/*.csv'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               column_names=column_headers,\n",
    "                                               num_epochs=1,\n",
    "                                               shuffle=True,\n",
    "                                               label_name=\"fare_amount\")\n",
    "    ds = ds.map(pack)\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args, _ = parse_args()\n",
    "    \n",
    "    print(args)\n",
    "    print(os.environ)\n",
    "\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "    train_dir = args.training\n",
    "    \n",
    "    ds = get_train_data(train_dir, batch_size)\n",
    "    \n",
    "    linear_model = LinearModel()\n",
    "    dnn_model = keras.Sequential(\n",
    "        [keras.layers.Dense(units=64), keras.layers.Dense(units=1)]\n",
    "    )\n",
    "    combined_model = WideDeepModel(linear_model, dnn_model)\n",
    "    combined_model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mse\"])\n",
    "    combined_model.fit(ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "taxi_experiment = Experiment.create(\n",
    "    experiment_name=f\"tf2-taxi-wide-deep-{int(time.time())}\",\n",
    "    description=\"Training wide and deep model on nyc taxi dataset\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "print(taxi_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "output_bucket = f\"s3://{bucket}/nyc-taxi/model/\"\n",
    "\n",
    "\n",
    "# Create unique job name with hyperparameter and time\n",
    "run_number = 1\n",
    "time_append = int(time.time())\n",
    "training_job_name = f\"tf2-taxi-wide-deep-training-{time_append}\"\n",
    "trial_name = f\"trial-tf2-taxi-wide-deep-training-{time_append}\"\n",
    "trial_desc = f\"tf2-taxi-wide-deep-run-{run_number}\"\n",
    "    \n",
    "\n",
    "# Create a new Trial and associate Tracker to it\n",
    "tf2_taxi_trial = Trial.create(\n",
    "    trial_name=trial_name,\n",
    "    experiment_name=taxi_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    "    tags=[{\"Key\": \"trial-desc\", \"Value\": trial_desc}],\n",
    ")\n",
    "\n",
    "\n",
    "# Create an experiment config that associates training job to the Trial\n",
    "experiment_config = {\n",
    "    \"ExperimentName\": taxi_experiment.experiment_name,\n",
    "    \"TrialName\": tf2_taxi_trial.trial_name,\n",
    "    \"TrialComponentDisplayName\": training_job_name,\n",
    "}\n",
    "\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"mse\", \"Regex\": \"mse: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"val_mse\", \"Regex\": \"val_mse: ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "\n",
    "spot_training=True\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    entry_point=\"tf_train.py\",\n",
    "    base_job_name=\"tf2-taxi-wide-deep\",\n",
    "    role=role,\n",
    "    use_spot_instances=spot_training,\n",
    "    max_run=1800,  # max training time in seconds\n",
    "    max_wait=1800,  # seconds to wait for spot instance\n",
    "    framework_version=\"2.6.2\",\n",
    "    py_version=\"py38\",\n",
    "    metric_definitions=metric_definitions,\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_bucket,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    hyperparameters={\"batch_size\": 512, \"epochs\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({\"training\": f\"s3://{data_bucket}/train/\", \"testing\": f\"s3://{data_bucket}/test/\"}, logs=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
